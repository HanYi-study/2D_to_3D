# 2d转3d的方法总结与对比

## 基于vision transformer架构的单目深度估计模型
使用的模型是DPT（Dense Prediction Transformer）模型
- 本质：基于 Vision Transformer（ViT）的单目深度估计模型。
- 输入：单张 RGB 图像。
- 输出：每个像素的深度值（深度图）。
- 主要功能：从单张图片直接预测场景的深度信息，适用于单目深度估计任务。
- 应用流程：深度图 → 点云 → 三维重建。

## 基于光照和灰度变化的 Shape from Shading（SFS）方法
使用的模型是MSGF-SFS（Multi-Scale Gradient Field Shape from Shading）模型
- 本质：一种基于光照和灰度变化的 Shape from Shading（SFS）方法。
- 输入：单张灰度或彩色图像，通常假设已知光照条件。
- 输出：物体表面的法向量场或深度图。
- 主要功能：通过分析图像的灰度变化和光照信息，推断物体表面的三维形状（通常是法向量或高度图）。
- 特点：MSGF-SFS 通过多尺度梯度场建模，提升了对细节和噪声的鲁棒性，适合细致表面重建。


> DPT与MSGF-SFS的区别：
>
| 方面 |	DPT（Transformer）单目深度估计 |	MSGF-SFS（Shape from Shading） |  
|--------|-------------------------------|--------------------------------|  
| 原理 |	深度学习，Transformer架构 |	传统图像处理，基于光照建模 |  
| 输入	| 单张RGB图像 |	单张图像+光照假设 |  
| 输出	| 深度图 | 	法向量场/深度图 |  
| 依赖 |	大规模数据训练 |	物理光照模型 |  
| 优势 |	适应性强，泛化能力好 |	对细节敏感，适合精细表面 |  
| 应用 |	通用场景深度估计 |	需要已知或可估计光照的场景 |  

---

<mark>有待验证：</mark>

##  **基于单张RGB图像的3D重建**
   - **Deemos的Hyper3D Rodin Gen-2**：采用"CAST"方法，从单张RGB图像重建组件对齐的3D场景，支持高保真3D资产生成，适用于游戏开发和虚拟世界构建。
   - **微软Copilot 3D**：免费工具，支持单张PNG/JPG图像输入，1分钟内生成GLB格式3D模型，适用于游戏开发、3D打印等场景。
   - **港科广团队的CVPR 2025技术**：从单张人体图像生成亚毫米级精度的3D模型，解决遮挡问题，适用于医疗和影视特效。
   - **Meta MCC方案**：基于RGB-D图像的3D重建，利用Transformer架构预测遮挡部分，适用于AR/VR内容生成。

##  **基于扩散模型的3D生成**
   - **Kiss3DGen**：将3D物体渲染为多视角RGB+法线图（"3D Bundle Image"），利用2D扩散模型生成3D资产，支持ControlNet扩展编辑功能。
   - **DreamFusion优化方法**：通过2D扩散模型优化3D表示（如NeRF），但依赖密集迭代，计算成本较高。

##  **神经辐射场（NeRF）相关技术**
   - **Instant NeRF**：NVIDIA的快速NeRF重建技术，从多张照片生成高质量3D场景，未来可能集成到Copilot 3D等工具中。
   - **Matrix-3D**：开源世界模型，从单图生成可导航3D空间，支持全景视频和3D场景重建。

##  **点云与网格生成方法**
   - **东卡罗来纳大学的3D高斯点云风格转换**：无需重建即可对3D高斯点云应用艺术风格，速度极快（1-2分钟）。
   - **Sparc3D**：结合稀疏可变形行进立方体（Sparcubes）和变分自编码器（VAE），实现高分辨率3D重建，适用于复杂几何形状。

##  **跨模态可控生成**
   - **浙大Img2CAD**：基于昇腾NPU，将草图/图像转换为CAD兼容的3D脚本语言，支持工业设计和3D打印。
   - **SpatialVLM/SpatialRGPT**：结合视觉语言模型（VLM）和深度信息，实现空间推理和3D场景理解。

## 6. **多模态与文本到3D生成**
   - **Deemos的Rodin Gen-2**：支持文本或图像输入生成可编辑的3D部件，进入"Vibe Modeling"时代。
   - **未来Copilot 3D方向**：计划支持文本到3D生成和多视角融合。
